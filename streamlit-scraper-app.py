import streamlit as st
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin, urlparse
import re
import time
from datetime import datetime, timedelta
import concurrent.futures
import json
import io

# --- ãƒ‡ãƒ¼ã‚¿å®šç¾© (å¤‰æ›´ãªã—) ---
PREFECTURES = {'æ±äº¬': 'tokyo', 'å¤§é˜ª': 'osaka', 'é¦™å·': 'kagawa', 'åŒ—æµ·é“': 'hokkaido', 'é’æ£®': 'aomori', 'å²©æ‰‹': 'iwate', 'å®®åŸ': 'miyagi', 'ç§‹ç”°': 'akita', 'å±±å½¢': 'yamagata', 'ç¦å³¶': 'fukushima', 'èŒ¨åŸ': 'ibaraki', 'æ ƒæœ¨': 'tochigi', 'ç¾¤é¦¬': 'gunma', 'åŸ¼ç‰': 'saitama', 'åƒè‘‰': 'chiba', 'ç¥å¥ˆå·': 'kanagawa', 'æ–°æ½Ÿ': 'niigata', 'å¯Œå±±': 'toyama', 'çŸ³å·': 'ishikawa', 'ç¦äº•': 'fui', 'å±±æ¢¨': 'yamanashi', 'é•·é‡': 'nagano', 'å²é˜œ': 'gifu', 'é™å²¡': 'shizuoka', 'æ„›çŸ¥': 'aichi', 'ä¸‰é‡': 'mie', 'æ»‹è³€': 'shiga', 'äº¬éƒ½': 'kyoto', 'å…µåº«': 'hyogo', 'å¥ˆè‰¯': 'nara', 'å’Œæ­Œå±±': 'wakayama', 'é³¥å–': 'tottori', 'å³¶æ ¹': 'shimane', 'å²¡å±±': 'okayama', 'åºƒå³¶': 'hiroshima', 'å±±å£': 'yamaguchi', 'å¾³å³¶': 'tokushima', 'æ„›åª›': 'ehime', 'é«˜çŸ¥': 'kochi', 'ç¦å²¡': 'fukuoka', 'ä½è³€': 'saga', 'é•·å´': 'nagasaki', 'ç†Šæœ¬': 'kumamoto', 'å¤§åˆ†': 'oita', 'å®®å´': 'miyazaki', 'é¹¿å…å³¶': 'kagoshima', 'æ²–ç¸„': 'okinawa'}
AGE_MAP = {'18ï½19æ­³': 'typ101', '20ï½24æ­³': 'typ102', '25ï½29æ­³': 'typ103'}
HEIGHT_MAP = {'149cmä»¥ä¸‹': 'typ201', '150ï½154cm': 'typ202', '155ï½159cm': 'typ203', '160ï½164cm': 'typ204'}
BUST_MAP = {'Aã‚«ãƒƒãƒ—': 'typ301', 'Bã‚«ãƒƒãƒ—': 'typ302', 'Cã‚«ãƒƒãƒ—': 'typ303', 'Dã‚«ãƒƒãƒ—': 'typ304', 'Eã‚«ãƒƒãƒ—': 'typ305', 'Fã‚«ãƒƒãƒ—': 'typ306', 'Gã‚«ãƒƒãƒ—': 'typ307', 'Hã‚«ãƒƒãƒ—': 'typ308', 'Iã‚«ãƒƒãƒ—ä»¥ä¸Š': 'typ309'}
FEATURE_MAP = {'ãƒ‹ãƒ¥ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹': 'typ601', 'ãŠåº—NO.1ãƒ»2ãƒ»3': 'typ602'}
ALL_TYPS = {**AGE_MAP, **HEIGHT_MAP, **BUST_MAP, **FEATURE_MAP}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ (å¤‰æ›´ãªã—) ---
def create_gallery_html(image_urls):
    if not isinstance(image_urls, list) or not image_urls: return ''
    image_paths_json = json.dumps(image_urls)
    return f'<div class="gallery-container" data-images=\'{image_paths_json}\' data-current-index="0" onclick="nextImage(this)" style="cursor: pointer;"><img src="{image_urls[0]}" width="120" onerror="this.style.display=\'none\'"></div>'

def create_star_rating_html(sort_number):
    try: num = int(sort_number)
    except (ValueError, TypeError): num = 999
    if num == 21: stars = "â˜…â˜…â˜…"
    elif num == 22: stars = "â˜…â˜…â˜†"
    elif num == 23: stars = "â˜…â˜†â˜†"
    else: stars = "â˜†â˜†â˜†"; num = 999
    sort_key_html = f'<span style="display: none;">{num:03d}</span>'
    star_display_html = f'<span style="font-size: 1.1em; color: #f2b01e;">{stars}</span>'
    return f'{sort_key_html}{star_display_html}'

def parse_style(style_text):
    data = {"èº«é•·(cm)": None, "ãƒã‚¹ãƒˆ(cm)": None, "ã‚«ãƒƒãƒ—": None, "ã‚¦ã‚§ã‚¹ãƒˆ(cm)": None, "ãƒ’ãƒƒãƒ—(cm)": None}
    if style_text and (match := re.search(r'T(\d+).*?ï½¥(\d+)\((.*?)\)ï½¥(\d+)ï½¥(\d+)', style_text)):
        try: data.update({"èº«é•·(cm)": int(match.group(1)), "ãƒã‚¹ãƒˆ(cm)": int(match.group(2)), "ã‚«ãƒƒãƒ—": match.group(3), "ã‚¦ã‚§ã‚¹ãƒˆ(cm)": int(match.group(4)), "ãƒ’ãƒƒãƒ—(cm)": int(match.group(5))})
        except (ValueError, IndexError): pass
    return data

def parse_age(age_text):
    if age_text and (match := re.search(r'\d+', age_text)): return int(match.group(0))
    return None

def parse_sortable_time(text):
    if text and (match := re.search(r'(\d{1,2}:\d{2})', text)): return match.group(1)
    return None

def get_girl_details(profile_url, session, headers):
    details = {"æœ¬æ—¥ã®å‡ºå‹¤äºˆå®š": None, "æ¬¡å›å‡ºå‹¤": None, "ã‚®ãƒ£ãƒ©ãƒªãƒ¼URL": [], "WEBäººæ°—ã®æ˜Ÿ": 999, "é€±åˆè¨ˆå‡ºå‹¤æ—¥æ•°": 0, "é€±åˆè¨ˆå‹¤å‹™æ™‚é–“": 0.0, "å£ã‚³ãƒŸæ•°": 0}
    if not profile_url: return details
    base_site_url = "https://www.cityheaven.net/"
    if (girlid_match := re.search(r'girlid-(\d+)', profile_url)) and (shop_url_match := re.search(r'(https?://.*?/girlid-)', profile_url)):
        girl_id, shop_base_url = girlid_match.group(1), shop_url_match.group(1).replace('girlid-', '')
        review_url = f"{shop_base_url}reviews/?girlid={girl_id}"
        try:
            review_res = session.get(review_url, timeout=10, headers=headers)
            if review_res.status_code == 200:
                review_soup = BeautifulSoup(review_res.content, 'html.parser')
                if (total_div := review_soup.find('div', class_='review-total')) and (count_match := re.search(r'(\d+)ä»¶', total_div.get_text())):
                    details["å£ã‚³ãƒŸæ•°"] = int(count_match.group(1))
        except requests.exceptions.RequestException: pass
    try:
        res = session.get(profile_url, timeout=30, headers=headers)
        if res.status_code != 200: return details
        soup = BeautifulSoup(res.content, 'html.parser')
        total_work_hours, total_work_days = 0.0, 0
        if schedule_list := soup.find('ul', id='girl_sukkin'):
            today_str = datetime.now().strftime("%-m/%-d")
            for item in schedule_list.find_all('li'):
                if (date_tag := item.find('dt')) and today_str in date_tag.get_text():
                    if dd_tag := item.find('dd'): details["æœ¬æ—¥ã®å‡ºå‹¤äºˆå®š"] = dd_tag.get_text(separator='-', strip=True)
                if dd_tag := item.find('dd'):
                    time_text = dd_tag.get_text(strip=True)
                    if match := re.search(r'(\d{1,2}:\d{1,2}).*?(\d{1,2}:\d{1,2})', time_text):
                        total_work_days += 1
                        try:
                            start_time, end_time = datetime.strptime(match.group(1), '%H:%M'), datetime.strptime(match.group(2), '%H:%M')
                            if end_time < start_time: end_time += timedelta(days=1)
                            total_work_hours += (end_time - start_time).total_seconds() / 3600
                        except ValueError: continue
        details["é€±åˆè¨ˆå‡ºå‹¤æ—¥æ•°"], details["é€±åˆè¨ˆå‹¤å‹™æ™‚é–“"] = total_work_days, total_work_hours
        if next_time_tag := soup.find('td', class_='shukkin-sugunavitext'):
            details["æ¬¡å›å‡ºå‹¤"] = parse_sortable_time(next_time_tag.get_text(strip=True))
        if star_img_tag := soup.find('img', class_='yoyaku_girlmark'):
            if match := re.search(r'yoyaku_(\d+)\.png', star_img_tag.get('src', '')):
                details["WEBäººæ°—ã®æ˜Ÿ"] = int(match.group(1))
        image_urls = []
        if photo_container := soup.find('div', class_='profile_photo'):
            for img_tag in photo_container.find_all('img'):
                if src := img_tag.get('src') or img_tag.get('data-src'):
                    if 'grpb' in src: image_urls.append(urljoin(base_site_url, src))
        if diary := soup.find('div', id='girlprofile_diary'):
            for item in diary.find_all('div', class_='thm'):
                if (img_tag := item.find('img')) and (src := img_tag.get('src')) and 'grdr' in src:
                    image_urls.append(urljoin(base_site_url, src))
        details["ã‚®ãƒ£ãƒ©ãƒªãƒ¼URL"] = list(dict.fromkeys(image_urls))[:6]
    except requests.exceptions.RequestException: pass
    return details

def run_scraper(params, progress_bar, status_text):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
    base_url = "https://www.cityheaven.net/"
    prefecture_path, filter_path, debug_mode, hide_inactive, page_limit = params
    target_path = f"{prefecture_path}/{filter_path}"
    all_girls_data = []
    with requests.Session() as session:
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter); session.mount("http://", adapter)
        try:
            status_text.text("ç·ãƒšãƒ¼ã‚¸æ•°ã‚’ç¢ºèªä¸­...")
            first_page_url = urljoin(base_url, target_path)
            response = session.get(first_page_url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            last_page = 1
            if pagination_div := soup.find('div', class_='shop_nav_list'):
                for link in pagination_div.find_all('a'):
                    if link.text.isdigit() and (page_num := int(link.text)) > last_page: last_page = page_num
            pages_to_scrape = 1 if debug_mode else last_page
            if page_limit: pages_to_scrape = min(pages_to_scrape, page_limit)
            initial_girl_list = []
            for page in range(1, pages_to_scrape + 1):
                status_text.text(f"ãƒšãƒ¼ã‚¸ {page}/{pages_to_scrape} ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—ä¸­...")
                progress_bar.progress(page / pages_to_scrape * 0.2)
                if page > 1:
                    response = session.get(urljoin(base_url, f"{target_path}{page}/"), timeout=30, headers=headers)
                    soup = BeautifulSoup(response.content, 'html.parser')
                for item in soup.find_all('li', class_='girls-list'):
                    if not item.find('div', class_='maingirl'): continue
                    status = (s.text.strip() if (s := item.find('span', 'soku')) else None) or "-"
                    profile_link = urljoin(base_url, p.get('href')) if (p := item.find('a', 'shopimg')) else None
                    data = {"åå‰": n.text.strip() if (n := item.find('p', 'girlname').find('a')) else "NoName", "å¹´é½¢": parse_age(a.text.strip() if (a := item.find('p', 'girlname').find('span')) else ''), "å‡ºå‹¤çŠ¶æ³": status, "æ¬¡å›å‡ºå‹¤": parse_sortable_time(status), **parse_style(s.text.strip() if (s := item.find('p', 'girlstyle')) else ''), "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯": profile_link, "åº—èˆ—å": (s.text.strip() if (s := item.find('p', 'shopname').find('a')) else None)}
                    initial_girl_list.append(data)
                time.sleep(0.5)
            status_text.text(f"å…¨{len(initial_girl_list)}äººã®è©³ç´°æƒ…å ±ã‚’ä¸¦åˆ—å–å¾—ä¸­...")
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                future_to_data = {executor.submit(get_girl_details, data['ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯'], session, headers): data for data in initial_girl_list}
                progress_count = 0
                for future in concurrent.futures.as_completed(future_to_data):
                    original_data = future_to_data[future]
                    try:
                        details = future.result()
                        original_data.update(details)
                        if original_data["æ¬¡å›å‡ºå‹¤"] is None: original_data["æ¬¡å›å‡ºå‹¤"] = details.get("æ¬¡å›å‡ºå‹¤")
                        all_girls_data.append(original_data)
                    except Exception: pass
                    progress_count += 1
                    progress_bar.progress(0.2 + (progress_count / len(initial_girl_list) * 0.8))
            if not all_girls_data:
                status_text.text("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return pd.DataFrame()
            df = pd.DataFrame(all_girls_data)
            if hide_inactive: df = df[df['æ¬¡å›å‡ºå‹¤'].notna()].copy()
            status_text.text("å‡¦ç†å®Œäº†ï¼"); return df
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); return pd.DataFrame()

# â˜…â˜…â˜… f-stringã®å•é¡Œã‚’ä¿®æ­£ã—ãŸHTMLç”Ÿæˆé–¢æ•° â˜…â˜…â˜…
def generate_html_report(df, title_text):
    if df.empty: return "<h1>ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“</h1>"
    df_display = df.copy()

    # (...ä¸­ç•¥... ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®ã‚³ãƒ¼ãƒ‰ã¯å¤‰æ›´ãªã—)
    df_display['ãƒã‚§ãƒƒã‚¯'] = '<input type="checkbox" class="row-checkbox" style="cursor:pointer;">'
    df_display['åå‰'] = df_display.apply(lambda row: f'<a href="{row["ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯"]}" target="_blank">{row["åå‰"]}</a>' if pd.notna(row['ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯']) else row['åå‰'], axis=1)
    df_display['ã‚®ãƒ£ãƒ©ãƒªãƒ¼'] = df_display['ã‚®ãƒ£ãƒ©ãƒªãƒ¼URL'].apply(create_gallery_html)
    df_display['WEBäººæ°—'] = df_display['WEBäººæ°—ã®æ˜Ÿ'].apply(create_star_rating_html)
    num_cols = ['å¹´é½¢', 'èº«é•·(cm)', 'ãƒã‚¹ãƒˆ(cm)', 'ã‚¦ã‚§ã‚¹ãƒˆ(cm)', 'ãƒ’ãƒƒãƒ—(cm)', 'å£ã‚³ãƒŸæ•°', 'é€±åˆè¨ˆå‡ºå‹¤æ—¥æ•°', 'é€±åˆè¨ˆå‹¤å‹™æ™‚é–“']
    for col in num_cols:
        if col in df_display.columns:
            df_display[col] = df_display[col].apply(lambda x: f"{x:.0f}" if pd.notna(x) and x > 0 else ("{:.1f}".format(x) if pd.notna(x) and x > 0 else ""))
    df_display.fillna("", inplace=True)
    rename_map = {"èº«é•·(cm)": "èº«é•·", "ãƒã‚¹ãƒˆ(cm)": "ãƒã‚¹ãƒˆ", "ã‚¦ã‚§ã‚¹ãƒˆ(cm)": "ã‚¦ã‚§ã‚¹ãƒˆ", "ãƒ’ãƒƒãƒ—(cm)": "ãƒ’ãƒƒãƒ—", "é€±åˆè¨ˆå‡ºå‹¤æ—¥æ•°": "å‡ºå‹¤æ—¥", "é€±åˆè¨ˆå‹¤å‹™æ™‚é–“": "å‹¤å‹™æ™‚é–“", "åº—èˆ—å": "åº—èˆ—"}
    df_display.rename(columns=rename_map, inplace=True)
    desired_order = ["ãƒã‚§ãƒƒã‚¯", "åå‰", "ã‚®ãƒ£ãƒ©ãƒªãƒ¼", "å¹´é½¢", "èº«é•·", "ã‚«ãƒƒãƒ—", "WEBäººæ°—", "å£ã‚³ãƒŸæ•°", "å‡ºå‹¤æ—¥", "å‹¤å‹™æ™‚é–“", "å‡ºå‹¤çŠ¶æ³", "æœ¬æ—¥ã®å‡ºå‹¤äºˆå®š", "æ¬¡å›å‡ºå‹¤", "ãƒã‚¹ãƒˆ", "ã‚¦ã‚§ã‚¹ãƒˆ", "ãƒ’ãƒƒãƒ—", "åº—èˆ—"]
    existing_columns = [col for col in desired_order if col in df_display.columns]
    df_display = df_display.reindex(columns=existing_columns)
    html_table = df_display.to_html(escape=False, index=False, table_id='resultsTable', classes='display compact stripe hover')

    # f-stringã§ã¯ãªãã€é€šå¸¸ã®æ–‡å­—åˆ—ã¨ã—ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
    html_template = """
    <html><head><meta charset="UTF-8"><title>{title}</title>
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.1/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.13.1/js/jquery.dataTables.js"></script>
    <style>body{{font-family:sans-serif; margin:1.5em;}} table.dataframe th,td{{padding:5px 10px;border:1px solid #ddd; text-align: left; vertical-align: middle;}} thead th{{background-color:#f0f0f0; cursor:pointer;}} .dataTables_wrapper .dataTables_length, .dataTables_wrapper .dataTables_filter {{float: left; margin-right: 20px;}} .dataTables_wrapper .dataTables_info {{clear: both; padding-top: 1em;}} #custom-filters {{padding: 10px; border: 1px solid #ccc; margin-bottom: 1em; border-radius: 5px;}} .filter-container > strong {{display: block; margin-bottom: 8px;}} .filter-row {{display: flex; align-items: center; flex-wrap: wrap; margin-bottom: 5px;}} .filter-row > span {{margin-right: 15px; margin-bottom: 5px;}} #custom-filters select, #custom-filters button {{padding: 4px; border: 1px solid #ccc; border-radius: 3px;}} .action-buttons {{ padding: 0 0 10px 0; }} .action-buttons button {{ padding: 5px 10px; margin-right: 10px; cursor: pointer; border: 1px solid #ccc; border-radius: 3px; background-color: #f0f0f0;}}</style>
    </head>
    <body><h1>{title}</h1><p>å„åˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã§ã‚½ãƒ¼ãƒˆã§ãã¾ã™ï¼ˆShift+ã‚¯ãƒªãƒƒã‚¯ã§è¤‡æ•°ã‚½ãƒ¼ãƒˆå¯ï¼‰ã€‚ç”»åƒã‚¯ãƒªãƒƒã‚¯ã§æ¬¡ã®å†™çœŸã«åˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã™ã€‚</p>
    <div id="custom-filters"><div class="filter-container"><strong>ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼</strong><div class="filter-row"><span>æ¬¡å›å‡ºå‹¤: <select id="min-time"></select> ï½ <select id="max-time"></select><button id="reset-time" type="button" style="margin-left: 5px; cursor: pointer;">ãƒªã‚»ãƒƒãƒˆ</button></span><span>å¹´é½¢: <select id="min-age"></select> ï½ <select id="max-age"></select></span><span>ã‚¦ã‚§ã‚¹ãƒˆ: <select id="min-waist"></select> ï½ <select id="max-waist"></select></span></div><div class="filter-row"><span>å£ã‚³ãƒŸæ•°: <select id="min-reviews"></select> ï½ <select id="max-reviews"></select><button id="reset-reviews" type="button" style="margin-left: 5px; cursor: pointer;">ãƒªã‚»ãƒƒãƒˆ</button></span><span>å‡ºå‹¤æ—¥: <select id="min-workdays"></select> ï½ <select id="max-workdays"></select><button id="reset-workdays" type="button" style="margin-left: 5px; cursor: pointer;">ãƒªã‚»ãƒƒãƒˆ</button></span></div></div></div>
    <div class="action-buttons"><button id="filter-checked-btn">ãƒã‚§ãƒƒã‚¯ã—ãŸã‚­ãƒ£ã‚¹ãƒˆã®ã¿ã‚’è¡¨ç¤º</button><button id="show-all-btn">å…¨ã¦ã®ã‚­ãƒ£ã‚¹ãƒˆã‚’è¡¨ç¤º</button></div>
    {table}
    <script>
    function nextImage(container) {{if (!container.dataset.images) return; const images = JSON.parse(container.dataset.images); if (images.length === 0) return; let currentIndex = parseInt(container.dataset.currentIndex, 10); currentIndex = (currentIndex + 1) % images.length; const imgTag = container.querySelector('img'); imgTag.src = images[currentIndex]; container.dataset.currentIndex = currentIndex;}}
    function setupAgeFilters() {{const ageOptions=['<option value="">æŒ‡å®šãªã—</option>']; for (let i=18; i<=35; i++) {{ageOptions.push(`<option value="${{i}}">${{i}}æ­³</option>`);}} $('#min-age, #max-age').html(ageOptions.join(''));}}
    function setupTimeFilters() {{const timeOptions=['<option value="">æŒ‡å®šãªã—</option>']; const hourSequence=[6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5]; for (const i of hourSequence) {{const hour=i.toString().padStart(2,'0'); timeOptions.push(`<option value="${{hour}}:00">${{hour}}:00</option>`);}} $('#min-time, #max-time').html(timeOptions.join(''));}}
    function setupWaistFilters() {{const waistOptions=['<option value="">æŒ‡å®šãªã—</option>']; for (let i=45; i<=80; i++) {{waistOptions.push(`<option value="${{i}}">${{i}}</option>`);}} $('#min-waist, #max-waist').html(waistOptions.join(''));}}
    function setupReviewFilters() {{const reviewOptions=['<option value="">æŒ‡å®šãªã—</option>']; const values=[1,2,3,5,10,20,30,50,100]; for (const val of values) {{reviewOptions.push(`<option value="${{val}}">${{val}}</option>`);}} $('#min-reviews, #max-reviews').html(reviewOptions.join(''));}}
    function setupWorkdayFilters() {{const workdayOptions=['<option value="">æŒ‡å®šãªã—</option>']; for (let i=0; i<=7; i++) {{workdayOptions.push(`<option value="${{i}}">${{i}}</option>`);}} $('#min-workdays, #max-workdays').html(workdayOptions.join(''));}}
    $(document).ready(function(){{
        let isCheckedFilterActive=false;
        $.fn.dataTable.ext.search.push(function(settings, data, dataIndex) {{
            const ageColIdx=3; const reviewColIdx=7; const workdayColIdx=8; const timeColIdx=12; const waistColIdx=14;
            var minAge=parseInt($('#min-age').val(),10); var maxAge=parseInt($('#max-age').val(),10); var cellAge=parseInt(data[ageColIdx],10);
            var minTime=$('#min-time').val(); var maxTime=$('#max-time').val(); var cellTime=data[timeColIdx]||"";
            var minWaist=parseInt($('#min-waist').val(),10); var maxWaist=parseInt($('#max-waist').val(),10); var cellWaist=parseInt(data[waistColIdx],10);
            var minReviews=parseInt($('#min-reviews').val(),10); var maxReviews=parseInt($('#max-reviews').val(),10); var cellReviews=parseInt(data[reviewColIdx],10);
            var minWorkdays=parseInt($('#min-workdays').val(),10); var maxWorkdays=parseInt($('#max-workdays').val(),10); var cellWorkdays=parseInt(data[workdayColIdx],10);
            if((!isNaN(minAge)&&(isNaN(cellAge)||cellAge<minAge))||(!isNaN(maxAge)&&(isNaN(cellAge)||cellAge>maxAge))) return false;
            if(minTime&&maxTime&&minTime>maxTime){{if(cellTime<minTime&&cellTime>maxTime)return false;}}else{{if((minTime&&cellTime<minTime)||(maxTime&&cellTime>maxTime))return false;}}
            if((!isNaN(minWaist)&&(isNaN(cellWaist)||cellWaist<minWaist))||(!isNaN(maxWaist)&&(isNaN(cellWaist)||cellWaist>maxWaist))) return false;
            if((!isNaN(minReviews)&&(isNaN(cellReviews)||cellReviews<minReviews))||(!isNaN(maxReviews)&&(isNaN(cellReviews)||cellReviews>maxReviews))) return false;
            if((!isNaN(minWorkdays)&&(isNaN(cellWorkdays)||cellWorkdays<minWorkdays))||(!isNaN(maxWorkdays)&&(isNaN(cellWorkdays)||cellWorkdays>maxWorkdays))) return false;
            if(isCheckedFilterActive){{var rowNode=table.row(dataIndex).node(); var isChecked=$(rowNode).find('.row-checkbox').is(':checked'); if(!isChecked)return false;}}
            return true;
        }});
        var table=$('#resultsTable').DataTable({{"pageLength":-1, "lengthMenu":[[10,25,50,100,-1],[10,25,50,100,"å…¨ã¦"]], "order":[], "dom":'<"top"lfi>rt<"bottom"p><"clear">'});
        setupAgeFilters(); setupTimeFilters(); setupWaistFilters(); setupReviewFilters(); setupWorkdayFilters();
        function setDefaultFiltersAndDraw(){{$('#min-age').val('18'); $('#max-age').val('29'); $('#min-waist').val('48'); $('#max-waist').val('60'); const now=new Date(); const startHour=(now.getHours()-3+24)%24; const formattedStart=startHour.toString().padStart(2,'0')+':00'; $('#min-time').val(formattedStart); $('#max-time').val('05:00'); table.draw();}}
        setDefaultFiltersAndDraw();
        $('#min-age, #max-age, #min-time, #max-time, #min-waist, #max-waist, #min-reviews, #max-reviews, #min-workdays, #max-workdays').on('change', function(){{table.draw();}});
        $('#reset-time').on('click', function(){{$('#min-time, #max-time').val(''); table.draw();}});
        $('#reset-reviews').on('click', function(){{$('#min-reviews, #max-reviews').val(''); table.draw();}});
        $('#reset-workdays').on('click', function(){{$('#min-workdays, #max-workdays').val(''); table.draw();}});
        $('#filter-checked-btn').on('click', function(){{isCheckedFilterActive=true; table.draw();}});
        $('#show-all-btn').on('click', function(){{isCheckedFilterActive=false; table.draw();}});
    }});
    </script></body></html>
    """
    # .format() ã‚’ä½¿ã£ã¦å¤‰æ•°ã‚’åŸ‹ã‚è¾¼ã‚€
    return html_template.format(title=title_text, table=html_table)


# --- Streamlit UI ---
st.set_page_config(page_title="CityHeaven Scraper", layout="wide")
st.title("ğŸ™ï¸ CityHeaven Scraper")

if 'result_df' not in st.session_state: st.session_state.result_df = None
if 'is_running' not in st.session_state: st.session_state.is_running = False

with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    prefecture_name = st.selectbox("éƒ½é“åºœçœŒ", options=list(PREFECTURES.keys()), index=0)
    
    # â˜…â˜…â˜… ã“ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¾ã—ãŸ â˜…â˜…â˜…
    with st.expander("ç‰¹å¾´ã§çµã‚Šè¾¼ã¿"):
        selected_features = []
        
        st.subheader("å¹´é½¢")
        for name in AGE_MAP.keys():
            if st.checkbox(name, value=True):
                selected_features.append(name)

        st.subheader("èº«é•·")
        for name in HEIGHT_MAP.keys():
            if st.checkbox(name, value=True):
                selected_features.append(name)
        
        st.subheader("ãƒã‚¹ãƒˆ")
        for name in BUST_MAP.keys():
            if st.checkbox(name):
                selected_features.append(name)
        
        st.subheader("ãã®ä»–ç‰¹å¾´")
        for name in FEATURE_MAP.keys():
            if st.checkbox(name):
                selected_features.append(name)

    st.header("ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    page_limit = st.select_slider("æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°", options=['å…¨ã¦', 1, 2, 3, 5, 10, 15, 20], value=3)
    hide_inactive = st.checkbox("å‡ºå‹¤æœªå®šè€…ã‚’è¡¨ç¤ºã—ãªã„", value=True)
    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ (1ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—)")
    start_button = st.button("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", disabled=st.session_state.is_running)

if start_button:
    st.session_state.is_running = True
    st.session_state.result_df = None
    selected_typ_codes = [ALL_TYPS[name] for name in selected_features]
    filter_path = f"girl-list/{'-'.join(selected_typ_codes)}/" if selected_typ_codes else "girl-list/"
    params = (PREFECTURES[prefecture_name], filter_path, debug_mode, hide_inactive, None if page_limit == 'å…¨ã¦' else int(page_limit))
    st.subheader("å‡¦ç†çŠ¶æ³"); progress_bar = st.progress(0); status_text = st.empty()
    result = run_scraper(params, progress_bar, status_text)
    st.session_state.result_df = result
    st.session_state.is_running = False
    st.rerun()

if st.session_state.result_df is not None:
    st.header("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ")
    if not st.session_state.result_df.empty:
        tab1, tab2 = st.tabs(["ğŸ“Š é«˜æ©Ÿèƒ½HTMLãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ“ˆ ã‚·ãƒ³ãƒ—ãƒ«ãªè¡¨"])
        with tab1:
            title = f"{prefecture_name} / {'ãƒ»'.join(selected_features) if selected_features else 'ç‰¹å¾´æŒ‡å®šãªã—'}"
            html_report = generate_html_report(st.session_state.result_df, title)
            st.components.v1.html(html_report, height=800, scrolling=True)
        with tab2:
            st.dataframe(st.session_state.result_df, column_config={"ã‚®ãƒ£ãƒ©ãƒªãƒ¼URL": st.column_config.ImageColumn("Photo"), "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯": st.column_config.LinkColumn("Profile Link")}, use_container_width=True)
        csv = st.session_state.result_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(label="çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name=f"heaven_data_{prefecture_name}_{int(time.time())}.csv", mime="text/csv")
    else:
        st.warning("æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
